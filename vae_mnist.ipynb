{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dyLdBqBy8DI9",
        "outputId": "58147ada-942a-4036-e893-dd64a8c80be9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 404: Not Found\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9.91M/9.91M [00:00<00:00, 39.5MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/train-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 404: Not Found\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 28.9k/28.9k [00:00<00:00, 1.22MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/train-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 404: Not Found\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1.65M/1.65M [00:00<00:00, 10.6MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 404: Not Found\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4.54k/4.54k [00:00<00:00, 6.25MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Epoch: 1 [0/60000 (0%)]\tLoss: 550.596191\n",
            "Train Epoch: 1 [1280/60000 (2%)]\tLoss: 306.821503\n",
            "Train Epoch: 1 [2560/60000 (4%)]\tLoss: 239.735825\n",
            "Train Epoch: 1 [3840/60000 (6%)]\tLoss: 219.265350\n",
            "Train Epoch: 1 [5120/60000 (9%)]\tLoss: 215.173080\n",
            "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 207.980484\n",
            "Train Epoch: 1 [7680/60000 (13%)]\tLoss: 205.002502\n",
            "Train Epoch: 1 [8960/60000 (15%)]\tLoss: 195.018494\n",
            "Train Epoch: 1 [10240/60000 (17%)]\tLoss: 195.951401\n",
            "Train Epoch: 1 [11520/60000 (19%)]\tLoss: 191.046783\n",
            "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 177.746719\n",
            "Train Epoch: 1 [14080/60000 (23%)]\tLoss: 173.476196\n",
            "Train Epoch: 1 [15360/60000 (26%)]\tLoss: 182.252762\n",
            "Train Epoch: 1 [16640/60000 (28%)]\tLoss: 168.943039\n",
            "Train Epoch: 1 [17920/60000 (30%)]\tLoss: 167.080704\n",
            "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 163.089142\n",
            "Train Epoch: 1 [20480/60000 (34%)]\tLoss: 161.482941\n",
            "Train Epoch: 1 [21760/60000 (36%)]\tLoss: 152.580902\n",
            "Train Epoch: 1 [23040/60000 (38%)]\tLoss: 156.822372\n",
            "Train Epoch: 1 [24320/60000 (41%)]\tLoss: 154.286804\n",
            "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 154.853241\n",
            "Train Epoch: 1 [26880/60000 (45%)]\tLoss: 150.406204\n",
            "Train Epoch: 1 [28160/60000 (47%)]\tLoss: 154.873779\n",
            "Train Epoch: 1 [29440/60000 (49%)]\tLoss: 147.339722\n",
            "Train Epoch: 1 [30720/60000 (51%)]\tLoss: 141.410065\n",
            "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 144.210587\n",
            "Train Epoch: 1 [33280/60000 (55%)]\tLoss: 145.480560\n",
            "Train Epoch: 1 [34560/60000 (58%)]\tLoss: 144.594559\n",
            "Train Epoch: 1 [35840/60000 (60%)]\tLoss: 136.373703\n",
            "Train Epoch: 1 [37120/60000 (62%)]\tLoss: 142.608658\n",
            "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 139.441650\n",
            "Train Epoch: 1 [39680/60000 (66%)]\tLoss: 134.518555\n",
            "Train Epoch: 1 [40960/60000 (68%)]\tLoss: 143.254944\n",
            "Train Epoch: 1 [42240/60000 (70%)]\tLoss: 140.900070\n",
            "Train Epoch: 1 [43520/60000 (72%)]\tLoss: 141.618835\n",
            "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 136.003036\n",
            "Train Epoch: 1 [46080/60000 (77%)]\tLoss: 141.084045\n",
            "Train Epoch: 1 [47360/60000 (79%)]\tLoss: 132.052063\n",
            "Train Epoch: 1 [48640/60000 (81%)]\tLoss: 132.223343\n",
            "Train Epoch: 1 [49920/60000 (83%)]\tLoss: 131.405838\n",
            "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 132.236023\n",
            "Train Epoch: 1 [52480/60000 (87%)]\tLoss: 132.012039\n",
            "Train Epoch: 1 [53760/60000 (90%)]\tLoss: 132.734344\n",
            "Train Epoch: 1 [55040/60000 (92%)]\tLoss: 127.142731\n",
            "Train Epoch: 1 [56320/60000 (94%)]\tLoss: 137.610321\n",
            "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 130.743805\n",
            "Train Epoch: 1 [58880/60000 (98%)]\tLoss: 133.279144\n",
            "====> Epoch: 1 Average loss: 164.8238\n",
            "====> Test set loss: 128.4076\n",
            "Train Epoch: 2 [0/60000 (0%)]\tLoss: 130.187012\n",
            "Train Epoch: 2 [1280/60000 (2%)]\tLoss: 130.928604\n",
            "Train Epoch: 2 [2560/60000 (4%)]\tLoss: 127.802902\n",
            "Train Epoch: 2 [3840/60000 (6%)]\tLoss: 133.428741\n",
            "Train Epoch: 2 [5120/60000 (9%)]\tLoss: 129.326462\n",
            "Train Epoch: 2 [6400/60000 (11%)]\tLoss: 125.969589\n",
            "Train Epoch: 2 [7680/60000 (13%)]\tLoss: 127.594131\n",
            "Train Epoch: 2 [8960/60000 (15%)]\tLoss: 122.670021\n",
            "Train Epoch: 2 [10240/60000 (17%)]\tLoss: 122.020905\n",
            "Train Epoch: 2 [11520/60000 (19%)]\tLoss: 130.053040\n",
            "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 126.389320\n",
            "Train Epoch: 2 [14080/60000 (23%)]\tLoss: 120.840027\n",
            "Train Epoch: 2 [15360/60000 (26%)]\tLoss: 123.787216\n",
            "Train Epoch: 2 [16640/60000 (28%)]\tLoss: 125.263855\n",
            "Train Epoch: 2 [17920/60000 (30%)]\tLoss: 130.278015\n",
            "Train Epoch: 2 [19200/60000 (32%)]\tLoss: 127.527359\n",
            "Train Epoch: 2 [20480/60000 (34%)]\tLoss: 128.394760\n",
            "Train Epoch: 2 [21760/60000 (36%)]\tLoss: 122.559433\n",
            "Train Epoch: 2 [23040/60000 (38%)]\tLoss: 125.598236\n",
            "Train Epoch: 2 [24320/60000 (41%)]\tLoss: 125.869308\n",
            "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 125.497360\n",
            "Train Epoch: 2 [26880/60000 (45%)]\tLoss: 119.243149\n",
            "Train Epoch: 2 [28160/60000 (47%)]\tLoss: 124.832031\n",
            "Train Epoch: 2 [29440/60000 (49%)]\tLoss: 123.905518\n",
            "Train Epoch: 2 [30720/60000 (51%)]\tLoss: 121.596085\n",
            "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 120.367424\n",
            "Train Epoch: 2 [33280/60000 (55%)]\tLoss: 121.267303\n",
            "Train Epoch: 2 [34560/60000 (58%)]\tLoss: 122.837219\n",
            "Train Epoch: 2 [35840/60000 (60%)]\tLoss: 117.205391\n",
            "Train Epoch: 2 [37120/60000 (62%)]\tLoss: 115.254364\n",
            "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 121.394714\n",
            "Train Epoch: 2 [39680/60000 (66%)]\tLoss: 120.933823\n",
            "Train Epoch: 2 [40960/60000 (68%)]\tLoss: 115.454666\n",
            "Train Epoch: 2 [42240/60000 (70%)]\tLoss: 119.539619\n",
            "Train Epoch: 2 [43520/60000 (72%)]\tLoss: 121.954422\n",
            "Train Epoch: 2 [44800/60000 (75%)]\tLoss: 120.611763\n",
            "Train Epoch: 2 [46080/60000 (77%)]\tLoss: 121.103615\n",
            "Train Epoch: 2 [47360/60000 (79%)]\tLoss: 118.720299\n",
            "Train Epoch: 2 [48640/60000 (81%)]\tLoss: 120.459167\n",
            "Train Epoch: 2 [49920/60000 (83%)]\tLoss: 121.167770\n",
            "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 121.203522\n",
            "Train Epoch: 2 [52480/60000 (87%)]\tLoss: 121.401505\n",
            "Train Epoch: 2 [53760/60000 (90%)]\tLoss: 119.636246\n",
            "Train Epoch: 2 [55040/60000 (92%)]\tLoss: 113.987045\n",
            "Train Epoch: 2 [56320/60000 (94%)]\tLoss: 116.728203\n",
            "Train Epoch: 2 [57600/60000 (96%)]\tLoss: 115.655418\n",
            "Train Epoch: 2 [58880/60000 (98%)]\tLoss: 121.002274\n",
            "====> Epoch: 2 Average loss: 122.2095\n",
            "====> Test set loss: 116.3062\n",
            "Train Epoch: 3 [0/60000 (0%)]\tLoss: 114.522400\n",
            "Train Epoch: 3 [1280/60000 (2%)]\tLoss: 113.399361\n",
            "Train Epoch: 3 [2560/60000 (4%)]\tLoss: 115.543777\n",
            "Train Epoch: 3 [3840/60000 (6%)]\tLoss: 118.735168\n",
            "Train Epoch: 3 [5120/60000 (9%)]\tLoss: 116.066399\n",
            "Train Epoch: 3 [6400/60000 (11%)]\tLoss: 116.745079\n",
            "Train Epoch: 3 [7680/60000 (13%)]\tLoss: 117.445320\n",
            "Train Epoch: 3 [8960/60000 (15%)]\tLoss: 116.234573\n",
            "Train Epoch: 3 [10240/60000 (17%)]\tLoss: 115.435822\n",
            "Train Epoch: 3 [11520/60000 (19%)]\tLoss: 118.560143\n",
            "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 115.639679\n",
            "Train Epoch: 3 [14080/60000 (23%)]\tLoss: 111.441467\n",
            "Train Epoch: 3 [15360/60000 (26%)]\tLoss: 115.183952\n",
            "Train Epoch: 3 [16640/60000 (28%)]\tLoss: 123.936691\n",
            "Train Epoch: 3 [17920/60000 (30%)]\tLoss: 119.041679\n",
            "Train Epoch: 3 [19200/60000 (32%)]\tLoss: 118.461098\n",
            "Train Epoch: 3 [20480/60000 (34%)]\tLoss: 117.088531\n",
            "Train Epoch: 3 [21760/60000 (36%)]\tLoss: 112.414017\n",
            "Train Epoch: 3 [23040/60000 (38%)]\tLoss: 113.722054\n",
            "Train Epoch: 3 [24320/60000 (41%)]\tLoss: 111.924370\n",
            "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 112.763092\n",
            "Train Epoch: 3 [26880/60000 (45%)]\tLoss: 114.587929\n",
            "Train Epoch: 3 [28160/60000 (47%)]\tLoss: 112.651810\n",
            "Train Epoch: 3 [29440/60000 (49%)]\tLoss: 111.719330\n",
            "Train Epoch: 3 [30720/60000 (51%)]\tLoss: 119.142845\n",
            "Train Epoch: 3 [32000/60000 (53%)]\tLoss: 112.727165\n",
            "Train Epoch: 3 [33280/60000 (55%)]\tLoss: 114.684700\n",
            "Train Epoch: 3 [34560/60000 (58%)]\tLoss: 115.441833\n",
            "Train Epoch: 3 [35840/60000 (60%)]\tLoss: 111.016754\n",
            "Train Epoch: 3 [37120/60000 (62%)]\tLoss: 119.120003\n",
            "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 115.020004\n",
            "Train Epoch: 3 [39680/60000 (66%)]\tLoss: 108.169540\n",
            "Train Epoch: 3 [40960/60000 (68%)]\tLoss: 116.241493\n",
            "Train Epoch: 3 [42240/60000 (70%)]\tLoss: 116.114029\n",
            "Train Epoch: 3 [43520/60000 (72%)]\tLoss: 116.260979\n",
            "Train Epoch: 3 [44800/60000 (75%)]\tLoss: 113.872696\n",
            "Train Epoch: 3 [46080/60000 (77%)]\tLoss: 115.787048\n",
            "Train Epoch: 3 [47360/60000 (79%)]\tLoss: 118.150436\n",
            "Train Epoch: 3 [48640/60000 (81%)]\tLoss: 112.786865\n",
            "Train Epoch: 3 [49920/60000 (83%)]\tLoss: 115.367340\n",
            "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 115.469910\n",
            "Train Epoch: 3 [52480/60000 (87%)]\tLoss: 113.279999\n",
            "Train Epoch: 3 [53760/60000 (90%)]\tLoss: 110.664558\n",
            "Train Epoch: 3 [55040/60000 (92%)]\tLoss: 109.686790\n",
            "Train Epoch: 3 [56320/60000 (94%)]\tLoss: 113.593094\n",
            "Train Epoch: 3 [57600/60000 (96%)]\tLoss: 107.432823\n",
            "Train Epoch: 3 [58880/60000 (98%)]\tLoss: 117.071236\n",
            "====> Epoch: 3 Average loss: 114.9473\n",
            "====> Test set loss: 112.1421\n",
            "Train Epoch: 4 [0/60000 (0%)]\tLoss: 109.716095\n",
            "Train Epoch: 4 [1280/60000 (2%)]\tLoss: 116.023117\n",
            "Train Epoch: 4 [2560/60000 (4%)]\tLoss: 113.598434\n",
            "Train Epoch: 4 [3840/60000 (6%)]\tLoss: 113.590866\n",
            "Train Epoch: 4 [5120/60000 (9%)]\tLoss: 113.623810\n",
            "Train Epoch: 4 [6400/60000 (11%)]\tLoss: 113.389351\n",
            "Train Epoch: 4 [7680/60000 (13%)]\tLoss: 113.514740\n",
            "Train Epoch: 4 [8960/60000 (15%)]\tLoss: 111.749023\n",
            "Train Epoch: 4 [10240/60000 (17%)]\tLoss: 111.784576\n",
            "Train Epoch: 4 [11520/60000 (19%)]\tLoss: 111.357941\n",
            "Train Epoch: 4 [12800/60000 (21%)]\tLoss: 113.803543\n",
            "Train Epoch: 4 [14080/60000 (23%)]\tLoss: 112.502373\n",
            "Train Epoch: 4 [15360/60000 (26%)]\tLoss: 116.449432\n",
            "Train Epoch: 4 [16640/60000 (28%)]\tLoss: 109.365181\n",
            "Train Epoch: 4 [17920/60000 (30%)]\tLoss: 111.284966\n",
            "Train Epoch: 4 [19200/60000 (32%)]\tLoss: 111.926422\n",
            "Train Epoch: 4 [20480/60000 (34%)]\tLoss: 109.789978\n",
            "Train Epoch: 4 [21760/60000 (36%)]\tLoss: 114.167206\n",
            "Train Epoch: 4 [23040/60000 (38%)]\tLoss: 110.427750\n",
            "Train Epoch: 4 [24320/60000 (41%)]\tLoss: 119.710770\n",
            "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 114.821320\n",
            "Train Epoch: 4 [26880/60000 (45%)]\tLoss: 114.219948\n",
            "Train Epoch: 4 [28160/60000 (47%)]\tLoss: 116.509155\n",
            "Train Epoch: 4 [29440/60000 (49%)]\tLoss: 113.776642\n",
            "Train Epoch: 4 [30720/60000 (51%)]\tLoss: 110.261093\n",
            "Train Epoch: 4 [32000/60000 (53%)]\tLoss: 108.852570\n",
            "Train Epoch: 4 [33280/60000 (55%)]\tLoss: 115.127487\n",
            "Train Epoch: 4 [34560/60000 (58%)]\tLoss: 113.619316\n",
            "Train Epoch: 4 [35840/60000 (60%)]\tLoss: 113.596283\n",
            "Train Epoch: 4 [37120/60000 (62%)]\tLoss: 108.088226\n",
            "Train Epoch: 4 [38400/60000 (64%)]\tLoss: 111.935547\n",
            "Train Epoch: 4 [39680/60000 (66%)]\tLoss: 114.545654\n",
            "Train Epoch: 4 [40960/60000 (68%)]\tLoss: 108.905540\n",
            "Train Epoch: 4 [42240/60000 (70%)]\tLoss: 112.203888\n",
            "Train Epoch: 4 [43520/60000 (72%)]\tLoss: 114.143219\n",
            "Train Epoch: 4 [44800/60000 (75%)]\tLoss: 110.751419\n",
            "Train Epoch: 4 [46080/60000 (77%)]\tLoss: 110.171593\n",
            "Train Epoch: 4 [47360/60000 (79%)]\tLoss: 114.779243\n",
            "Train Epoch: 4 [48640/60000 (81%)]\tLoss: 113.491913\n",
            "Train Epoch: 4 [49920/60000 (83%)]\tLoss: 111.279305\n",
            "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 107.221970\n",
            "Train Epoch: 4 [52480/60000 (87%)]\tLoss: 111.942993\n",
            "Train Epoch: 4 [53760/60000 (90%)]\tLoss: 107.129333\n",
            "Train Epoch: 4 [55040/60000 (92%)]\tLoss: 109.478058\n",
            "Train Epoch: 4 [56320/60000 (94%)]\tLoss: 106.006493\n",
            "Train Epoch: 4 [57600/60000 (96%)]\tLoss: 113.217216\n",
            "Train Epoch: 4 [58880/60000 (98%)]\tLoss: 112.751160\n",
            "====> Epoch: 4 Average loss: 111.8147\n",
            "====> Test set loss: 109.8286\n",
            "Train Epoch: 5 [0/60000 (0%)]\tLoss: 108.789444\n",
            "Train Epoch: 5 [1280/60000 (2%)]\tLoss: 109.059891\n",
            "Train Epoch: 5 [2560/60000 (4%)]\tLoss: 112.343384\n",
            "Train Epoch: 5 [3840/60000 (6%)]\tLoss: 110.128044\n",
            "Train Epoch: 5 [5120/60000 (9%)]\tLoss: 106.779747\n",
            "Train Epoch: 5 [6400/60000 (11%)]\tLoss: 106.072395\n",
            "Train Epoch: 5 [7680/60000 (13%)]\tLoss: 110.528610\n",
            "Train Epoch: 5 [8960/60000 (15%)]\tLoss: 112.641632\n",
            "Train Epoch: 5 [10240/60000 (17%)]\tLoss: 106.428452\n",
            "Train Epoch: 5 [11520/60000 (19%)]\tLoss: 110.463638\n",
            "Train Epoch: 5 [12800/60000 (21%)]\tLoss: 107.585716\n",
            "Train Epoch: 5 [14080/60000 (23%)]\tLoss: 112.705513\n",
            "Train Epoch: 5 [15360/60000 (26%)]\tLoss: 112.603333\n",
            "Train Epoch: 5 [16640/60000 (28%)]\tLoss: 106.006531\n",
            "Train Epoch: 5 [17920/60000 (30%)]\tLoss: 109.399788\n",
            "Train Epoch: 5 [19200/60000 (32%)]\tLoss: 109.182358\n",
            "Train Epoch: 5 [20480/60000 (34%)]\tLoss: 110.329086\n",
            "Train Epoch: 5 [21760/60000 (36%)]\tLoss: 111.247528\n",
            "Train Epoch: 5 [23040/60000 (38%)]\tLoss: 104.787109\n",
            "Train Epoch: 5 [24320/60000 (41%)]\tLoss: 112.228317\n",
            "Train Epoch: 5 [25600/60000 (43%)]\tLoss: 111.842056\n",
            "Train Epoch: 5 [26880/60000 (45%)]\tLoss: 108.316544\n",
            "Train Epoch: 5 [28160/60000 (47%)]\tLoss: 108.399864\n",
            "Train Epoch: 5 [29440/60000 (49%)]\tLoss: 111.127289\n",
            "Train Epoch: 5 [30720/60000 (51%)]\tLoss: 111.147057\n",
            "Train Epoch: 5 [32000/60000 (53%)]\tLoss: 107.255295\n",
            "Train Epoch: 5 [33280/60000 (55%)]\tLoss: 107.524261\n",
            "Train Epoch: 5 [34560/60000 (58%)]\tLoss: 104.843193\n",
            "Train Epoch: 5 [35840/60000 (60%)]\tLoss: 109.793129\n",
            "Train Epoch: 5 [37120/60000 (62%)]\tLoss: 111.724037\n",
            "Train Epoch: 5 [38400/60000 (64%)]\tLoss: 112.909111\n",
            "Train Epoch: 5 [39680/60000 (66%)]\tLoss: 113.519974\n",
            "Train Epoch: 5 [40960/60000 (68%)]\tLoss: 108.612541\n",
            "Train Epoch: 5 [42240/60000 (70%)]\tLoss: 106.717880\n",
            "Train Epoch: 5 [43520/60000 (72%)]\tLoss: 108.022308\n",
            "Train Epoch: 5 [44800/60000 (75%)]\tLoss: 107.621796\n",
            "Train Epoch: 5 [46080/60000 (77%)]\tLoss: 108.554794\n",
            "Train Epoch: 5 [47360/60000 (79%)]\tLoss: 112.186905\n",
            "Train Epoch: 5 [48640/60000 (81%)]\tLoss: 109.368515\n",
            "Train Epoch: 5 [49920/60000 (83%)]\tLoss: 109.742813\n",
            "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 109.698845\n",
            "Train Epoch: 5 [52480/60000 (87%)]\tLoss: 108.978859\n",
            "Train Epoch: 5 [53760/60000 (90%)]\tLoss: 111.203568\n",
            "Train Epoch: 5 [55040/60000 (92%)]\tLoss: 110.398239\n",
            "Train Epoch: 5 [56320/60000 (94%)]\tLoss: 110.004837\n",
            "Train Epoch: 5 [57600/60000 (96%)]\tLoss: 110.227089\n",
            "Train Epoch: 5 [58880/60000 (98%)]\tLoss: 109.597397\n",
            "====> Epoch: 5 Average loss: 109.8975\n",
            "====> Test set loss: 108.2243\n",
            "Train Epoch: 6 [0/60000 (0%)]\tLoss: 103.290993\n",
            "Train Epoch: 6 [1280/60000 (2%)]\tLoss: 110.683929\n",
            "Train Epoch: 6 [2560/60000 (4%)]\tLoss: 106.791138\n",
            "Train Epoch: 6 [3840/60000 (6%)]\tLoss: 106.751259\n",
            "Train Epoch: 6 [5120/60000 (9%)]\tLoss: 106.095558\n",
            "Train Epoch: 6 [6400/60000 (11%)]\tLoss: 112.282784\n",
            "Train Epoch: 6 [7680/60000 (13%)]\tLoss: 108.517349\n",
            "Train Epoch: 6 [8960/60000 (15%)]\tLoss: 106.734489\n",
            "Train Epoch: 6 [10240/60000 (17%)]\tLoss: 111.934433\n",
            "Train Epoch: 6 [11520/60000 (19%)]\tLoss: 110.771492\n",
            "Train Epoch: 6 [12800/60000 (21%)]\tLoss: 108.105194\n",
            "Train Epoch: 6 [14080/60000 (23%)]\tLoss: 104.932640\n",
            "Train Epoch: 6 [15360/60000 (26%)]\tLoss: 110.307999\n",
            "Train Epoch: 6 [16640/60000 (28%)]\tLoss: 107.274384\n",
            "Train Epoch: 6 [17920/60000 (30%)]\tLoss: 101.185860\n",
            "Train Epoch: 6 [19200/60000 (32%)]\tLoss: 109.029510\n",
            "Train Epoch: 6 [20480/60000 (34%)]\tLoss: 112.206268\n",
            "Train Epoch: 6 [21760/60000 (36%)]\tLoss: 107.894913\n",
            "Train Epoch: 6 [23040/60000 (38%)]\tLoss: 111.945847\n",
            "Train Epoch: 6 [24320/60000 (41%)]\tLoss: 113.081184\n",
            "Train Epoch: 6 [25600/60000 (43%)]\tLoss: 104.936584\n",
            "Train Epoch: 6 [26880/60000 (45%)]\tLoss: 108.515663\n",
            "Train Epoch: 6 [28160/60000 (47%)]\tLoss: 107.344223\n",
            "Train Epoch: 6 [29440/60000 (49%)]\tLoss: 111.984055\n",
            "Train Epoch: 6 [30720/60000 (51%)]\tLoss: 104.905296\n",
            "Train Epoch: 6 [32000/60000 (53%)]\tLoss: 112.847977\n",
            "Train Epoch: 6 [33280/60000 (55%)]\tLoss: 108.246948\n",
            "Train Epoch: 6 [34560/60000 (58%)]\tLoss: 106.982819\n",
            "Train Epoch: 6 [35840/60000 (60%)]\tLoss: 106.877869\n",
            "Train Epoch: 6 [37120/60000 (62%)]\tLoss: 105.180420\n",
            "Train Epoch: 6 [38400/60000 (64%)]\tLoss: 112.689590\n",
            "Train Epoch: 6 [39680/60000 (66%)]\tLoss: 108.410927\n",
            "Train Epoch: 6 [40960/60000 (68%)]\tLoss: 106.230644\n",
            "Train Epoch: 6 [42240/60000 (70%)]\tLoss: 107.837769\n",
            "Train Epoch: 6 [43520/60000 (72%)]\tLoss: 111.593208\n",
            "Train Epoch: 6 [44800/60000 (75%)]\tLoss: 105.954422\n",
            "Train Epoch: 6 [46080/60000 (77%)]\tLoss: 107.439941\n",
            "Train Epoch: 6 [47360/60000 (79%)]\tLoss: 108.032181\n",
            "Train Epoch: 6 [48640/60000 (81%)]\tLoss: 106.468689\n",
            "Train Epoch: 6 [49920/60000 (83%)]\tLoss: 111.077057\n",
            "Train Epoch: 6 [51200/60000 (85%)]\tLoss: 106.736771\n",
            "Train Epoch: 6 [52480/60000 (87%)]\tLoss: 102.143814\n",
            "Train Epoch: 6 [53760/60000 (90%)]\tLoss: 106.727974\n",
            "Train Epoch: 6 [55040/60000 (92%)]\tLoss: 109.930840\n",
            "Train Epoch: 6 [56320/60000 (94%)]\tLoss: 105.671814\n",
            "Train Epoch: 6 [57600/60000 (96%)]\tLoss: 107.608231\n",
            "Train Epoch: 6 [58880/60000 (98%)]\tLoss: 105.486633\n",
            "====> Epoch: 6 Average loss: 108.7672\n",
            "====> Test set loss: 107.4911\n",
            "Train Epoch: 7 [0/60000 (0%)]\tLoss: 109.110870\n",
            "Train Epoch: 7 [1280/60000 (2%)]\tLoss: 107.447449\n",
            "Train Epoch: 7 [2560/60000 (4%)]\tLoss: 105.081924\n",
            "Train Epoch: 7 [3840/60000 (6%)]\tLoss: 105.185631\n",
            "Train Epoch: 7 [5120/60000 (9%)]\tLoss: 106.269577\n",
            "Train Epoch: 7 [6400/60000 (11%)]\tLoss: 105.510330\n",
            "Train Epoch: 7 [7680/60000 (13%)]\tLoss: 103.944107\n",
            "Train Epoch: 7 [8960/60000 (15%)]\tLoss: 107.696594\n",
            "Train Epoch: 7 [10240/60000 (17%)]\tLoss: 110.987457\n",
            "Train Epoch: 7 [11520/60000 (19%)]\tLoss: 111.218925\n",
            "Train Epoch: 7 [12800/60000 (21%)]\tLoss: 107.276848\n",
            "Train Epoch: 7 [14080/60000 (23%)]\tLoss: 107.506607\n",
            "Train Epoch: 7 [15360/60000 (26%)]\tLoss: 108.756760\n",
            "Train Epoch: 7 [16640/60000 (28%)]\tLoss: 110.460083\n",
            "Train Epoch: 7 [17920/60000 (30%)]\tLoss: 105.966850\n",
            "Train Epoch: 7 [19200/60000 (32%)]\tLoss: 108.264420\n",
            "Train Epoch: 7 [20480/60000 (34%)]\tLoss: 108.323692\n",
            "Train Epoch: 7 [21760/60000 (36%)]\tLoss: 106.882423\n",
            "Train Epoch: 7 [23040/60000 (38%)]\tLoss: 106.314621\n",
            "Train Epoch: 7 [24320/60000 (41%)]\tLoss: 105.185715\n",
            "Train Epoch: 7 [25600/60000 (43%)]\tLoss: 103.709000\n",
            "Train Epoch: 7 [26880/60000 (45%)]\tLoss: 105.870781\n",
            "Train Epoch: 7 [28160/60000 (47%)]\tLoss: 106.571274\n",
            "Train Epoch: 7 [29440/60000 (49%)]\tLoss: 111.015755\n",
            "Train Epoch: 7 [30720/60000 (51%)]\tLoss: 107.781815\n",
            "Train Epoch: 7 [32000/60000 (53%)]\tLoss: 104.939568\n",
            "Train Epoch: 7 [33280/60000 (55%)]\tLoss: 104.456245\n",
            "Train Epoch: 7 [34560/60000 (58%)]\tLoss: 103.694275\n",
            "Train Epoch: 7 [35840/60000 (60%)]\tLoss: 104.240662\n",
            "Train Epoch: 7 [37120/60000 (62%)]\tLoss: 109.199081\n",
            "Train Epoch: 7 [38400/60000 (64%)]\tLoss: 109.879333\n",
            "Train Epoch: 7 [39680/60000 (66%)]\tLoss: 102.790596\n",
            "Train Epoch: 7 [40960/60000 (68%)]\tLoss: 111.671875\n",
            "Train Epoch: 7 [42240/60000 (70%)]\tLoss: 106.671051\n",
            "Train Epoch: 7 [43520/60000 (72%)]\tLoss: 105.331276\n",
            "Train Epoch: 7 [44800/60000 (75%)]\tLoss: 106.613708\n",
            "Train Epoch: 7 [46080/60000 (77%)]\tLoss: 107.338882\n",
            "Train Epoch: 7 [47360/60000 (79%)]\tLoss: 108.043350\n",
            "Train Epoch: 7 [48640/60000 (81%)]\tLoss: 104.618935\n",
            "Train Epoch: 7 [49920/60000 (83%)]\tLoss: 110.237106\n",
            "Train Epoch: 7 [51200/60000 (85%)]\tLoss: 108.869690\n",
            "Train Epoch: 7 [52480/60000 (87%)]\tLoss: 106.602821\n",
            "Train Epoch: 7 [53760/60000 (90%)]\tLoss: 108.428764\n",
            "Train Epoch: 7 [55040/60000 (92%)]\tLoss: 110.395554\n",
            "Train Epoch: 7 [56320/60000 (94%)]\tLoss: 107.527809\n",
            "Train Epoch: 7 [57600/60000 (96%)]\tLoss: 107.293411\n",
            "Train Epoch: 7 [58880/60000 (98%)]\tLoss: 105.231400\n",
            "====> Epoch: 7 Average loss: 107.8462\n",
            "====> Test set loss: 106.8127\n",
            "Train Epoch: 8 [0/60000 (0%)]\tLoss: 110.913315\n",
            "Train Epoch: 8 [1280/60000 (2%)]\tLoss: 107.537285\n",
            "Train Epoch: 8 [2560/60000 (4%)]\tLoss: 105.345596\n",
            "Train Epoch: 8 [3840/60000 (6%)]\tLoss: 106.015137\n",
            "Train Epoch: 8 [5120/60000 (9%)]\tLoss: 104.179123\n",
            "Train Epoch: 8 [6400/60000 (11%)]\tLoss: 103.824593\n",
            "Train Epoch: 8 [7680/60000 (13%)]\tLoss: 109.482613\n",
            "Train Epoch: 8 [8960/60000 (15%)]\tLoss: 108.100067\n",
            "Train Epoch: 8 [10240/60000 (17%)]\tLoss: 109.251411\n",
            "Train Epoch: 8 [11520/60000 (19%)]\tLoss: 109.647919\n",
            "Train Epoch: 8 [12800/60000 (21%)]\tLoss: 108.783630\n",
            "Train Epoch: 8 [14080/60000 (23%)]\tLoss: 104.409943\n",
            "Train Epoch: 8 [15360/60000 (26%)]\tLoss: 103.970535\n",
            "Train Epoch: 8 [16640/60000 (28%)]\tLoss: 107.253433\n",
            "Train Epoch: 8 [17920/60000 (30%)]\tLoss: 106.716362\n",
            "Train Epoch: 8 [19200/60000 (32%)]\tLoss: 108.549255\n",
            "Train Epoch: 8 [20480/60000 (34%)]\tLoss: 106.757256\n",
            "Train Epoch: 8 [21760/60000 (36%)]\tLoss: 108.120621\n",
            "Train Epoch: 8 [23040/60000 (38%)]\tLoss: 104.140930\n",
            "Train Epoch: 8 [24320/60000 (41%)]\tLoss: 107.824646\n",
            "Train Epoch: 8 [25600/60000 (43%)]\tLoss: 110.710861\n",
            "Train Epoch: 8 [26880/60000 (45%)]\tLoss: 103.936661\n",
            "Train Epoch: 8 [28160/60000 (47%)]\tLoss: 106.290062\n",
            "Train Epoch: 8 [29440/60000 (49%)]\tLoss: 112.281921\n",
            "Train Epoch: 8 [30720/60000 (51%)]\tLoss: 107.562256\n",
            "Train Epoch: 8 [32000/60000 (53%)]\tLoss: 106.040878\n",
            "Train Epoch: 8 [33280/60000 (55%)]\tLoss: 108.132973\n",
            "Train Epoch: 8 [34560/60000 (58%)]\tLoss: 108.452805\n",
            "Train Epoch: 8 [35840/60000 (60%)]\tLoss: 109.118576\n",
            "Train Epoch: 8 [37120/60000 (62%)]\tLoss: 107.897003\n",
            "Train Epoch: 8 [38400/60000 (64%)]\tLoss: 105.271362\n",
            "Train Epoch: 8 [39680/60000 (66%)]\tLoss: 105.720482\n",
            "Train Epoch: 8 [40960/60000 (68%)]\tLoss: 105.709709\n",
            "Train Epoch: 8 [42240/60000 (70%)]\tLoss: 102.534065\n",
            "Train Epoch: 8 [43520/60000 (72%)]\tLoss: 104.966049\n",
            "Train Epoch: 8 [44800/60000 (75%)]\tLoss: 110.326492\n",
            "Train Epoch: 8 [46080/60000 (77%)]\tLoss: 102.245697\n",
            "Train Epoch: 8 [47360/60000 (79%)]\tLoss: 103.798141\n",
            "Train Epoch: 8 [48640/60000 (81%)]\tLoss: 107.641663\n",
            "Train Epoch: 8 [49920/60000 (83%)]\tLoss: 103.443466\n",
            "Train Epoch: 8 [51200/60000 (85%)]\tLoss: 110.789467\n",
            "Train Epoch: 8 [52480/60000 (87%)]\tLoss: 108.033043\n",
            "Train Epoch: 8 [53760/60000 (90%)]\tLoss: 109.354462\n",
            "Train Epoch: 8 [55040/60000 (92%)]\tLoss: 108.674194\n",
            "Train Epoch: 8 [56320/60000 (94%)]\tLoss: 103.792389\n",
            "Train Epoch: 8 [57600/60000 (96%)]\tLoss: 106.024597\n",
            "Train Epoch: 8 [58880/60000 (98%)]\tLoss: 106.034691\n",
            "====> Epoch: 8 Average loss: 107.2098\n",
            "====> Test set loss: 106.4537\n",
            "Train Epoch: 9 [0/60000 (0%)]\tLoss: 107.942131\n",
            "Train Epoch: 9 [1280/60000 (2%)]\tLoss: 108.460129\n",
            "Train Epoch: 9 [2560/60000 (4%)]\tLoss: 111.857590\n",
            "Train Epoch: 9 [3840/60000 (6%)]\tLoss: 108.580627\n",
            "Train Epoch: 9 [5120/60000 (9%)]\tLoss: 103.975937\n",
            "Train Epoch: 9 [6400/60000 (11%)]\tLoss: 102.682533\n",
            "Train Epoch: 9 [7680/60000 (13%)]\tLoss: 104.396423\n",
            "Train Epoch: 9 [8960/60000 (15%)]\tLoss: 106.346794\n",
            "Train Epoch: 9 [10240/60000 (17%)]\tLoss: 108.113106\n",
            "Train Epoch: 9 [11520/60000 (19%)]\tLoss: 109.396126\n",
            "Train Epoch: 9 [12800/60000 (21%)]\tLoss: 107.142715\n",
            "Train Epoch: 9 [14080/60000 (23%)]\tLoss: 107.638603\n",
            "Train Epoch: 9 [15360/60000 (26%)]\tLoss: 102.416023\n",
            "Train Epoch: 9 [16640/60000 (28%)]\tLoss: 105.948540\n",
            "Train Epoch: 9 [17920/60000 (30%)]\tLoss: 106.433578\n",
            "Train Epoch: 9 [19200/60000 (32%)]\tLoss: 105.823822\n",
            "Train Epoch: 9 [20480/60000 (34%)]\tLoss: 111.211243\n",
            "Train Epoch: 9 [21760/60000 (36%)]\tLoss: 105.933426\n",
            "Train Epoch: 9 [23040/60000 (38%)]\tLoss: 112.107376\n",
            "Train Epoch: 9 [24320/60000 (41%)]\tLoss: 108.283348\n",
            "Train Epoch: 9 [25600/60000 (43%)]\tLoss: 107.196083\n",
            "Train Epoch: 9 [26880/60000 (45%)]\tLoss: 109.429893\n",
            "Train Epoch: 9 [28160/60000 (47%)]\tLoss: 108.546875\n",
            "Train Epoch: 9 [29440/60000 (49%)]\tLoss: 108.695396\n",
            "Train Epoch: 9 [30720/60000 (51%)]\tLoss: 110.813255\n",
            "Train Epoch: 9 [32000/60000 (53%)]\tLoss: 109.549545\n",
            "Train Epoch: 9 [33280/60000 (55%)]\tLoss: 107.058624\n",
            "Train Epoch: 9 [34560/60000 (58%)]\tLoss: 106.749397\n",
            "Train Epoch: 9 [35840/60000 (60%)]\tLoss: 111.219284\n",
            "Train Epoch: 9 [37120/60000 (62%)]\tLoss: 107.962013\n",
            "Train Epoch: 9 [38400/60000 (64%)]\tLoss: 104.075317\n",
            "Train Epoch: 9 [39680/60000 (66%)]\tLoss: 108.558533\n",
            "Train Epoch: 9 [40960/60000 (68%)]\tLoss: 105.444580\n",
            "Train Epoch: 9 [42240/60000 (70%)]\tLoss: 106.594795\n",
            "Train Epoch: 9 [43520/60000 (72%)]\tLoss: 108.848991\n",
            "Train Epoch: 9 [44800/60000 (75%)]\tLoss: 106.772316\n",
            "Train Epoch: 9 [46080/60000 (77%)]\tLoss: 105.491188\n",
            "Train Epoch: 9 [47360/60000 (79%)]\tLoss: 108.410454\n",
            "Train Epoch: 9 [48640/60000 (81%)]\tLoss: 103.433434\n",
            "Train Epoch: 9 [49920/60000 (83%)]\tLoss: 104.864937\n",
            "Train Epoch: 9 [51200/60000 (85%)]\tLoss: 105.370956\n",
            "Train Epoch: 9 [52480/60000 (87%)]\tLoss: 102.651787\n",
            "Train Epoch: 9 [53760/60000 (90%)]\tLoss: 107.732162\n",
            "Train Epoch: 9 [55040/60000 (92%)]\tLoss: 107.068436\n",
            "Train Epoch: 9 [56320/60000 (94%)]\tLoss: 104.744514\n",
            "Train Epoch: 9 [57600/60000 (96%)]\tLoss: 109.334503\n",
            "Train Epoch: 9 [58880/60000 (98%)]\tLoss: 108.618210\n",
            "====> Epoch: 9 Average loss: 106.6375\n",
            "====> Test set loss: 105.8565\n",
            "Train Epoch: 10 [0/60000 (0%)]\tLoss: 102.055244\n",
            "Train Epoch: 10 [1280/60000 (2%)]\tLoss: 104.099159\n",
            "Train Epoch: 10 [2560/60000 (4%)]\tLoss: 102.574402\n",
            "Train Epoch: 10 [3840/60000 (6%)]\tLoss: 106.483521\n",
            "Train Epoch: 10 [5120/60000 (9%)]\tLoss: 110.118858\n",
            "Train Epoch: 10 [6400/60000 (11%)]\tLoss: 108.359573\n",
            "Train Epoch: 10 [7680/60000 (13%)]\tLoss: 104.105782\n",
            "Train Epoch: 10 [8960/60000 (15%)]\tLoss: 104.841103\n",
            "Train Epoch: 10 [10240/60000 (17%)]\tLoss: 104.370590\n",
            "Train Epoch: 10 [11520/60000 (19%)]\tLoss: 103.156021\n",
            "Train Epoch: 10 [12800/60000 (21%)]\tLoss: 108.136200\n",
            "Train Epoch: 10 [14080/60000 (23%)]\tLoss: 104.029602\n",
            "Train Epoch: 10 [15360/60000 (26%)]\tLoss: 104.632523\n",
            "Train Epoch: 10 [16640/60000 (28%)]\tLoss: 103.298233\n",
            "Train Epoch: 10 [17920/60000 (30%)]\tLoss: 107.183327\n",
            "Train Epoch: 10 [19200/60000 (32%)]\tLoss: 106.468742\n",
            "Train Epoch: 10 [20480/60000 (34%)]\tLoss: 102.211441\n",
            "Train Epoch: 10 [21760/60000 (36%)]\tLoss: 106.886826\n",
            "Train Epoch: 10 [23040/60000 (38%)]\tLoss: 110.294067\n",
            "Train Epoch: 10 [24320/60000 (41%)]\tLoss: 107.144760\n",
            "Train Epoch: 10 [25600/60000 (43%)]\tLoss: 106.484177\n",
            "Train Epoch: 10 [26880/60000 (45%)]\tLoss: 109.492218\n",
            "Train Epoch: 10 [28160/60000 (47%)]\tLoss: 108.044693\n",
            "Train Epoch: 10 [29440/60000 (49%)]\tLoss: 104.452789\n",
            "Train Epoch: 10 [30720/60000 (51%)]\tLoss: 106.636436\n",
            "Train Epoch: 10 [32000/60000 (53%)]\tLoss: 112.021194\n",
            "Train Epoch: 10 [33280/60000 (55%)]\tLoss: 108.123459\n",
            "Train Epoch: 10 [34560/60000 (58%)]\tLoss: 110.173416\n",
            "Train Epoch: 10 [35840/60000 (60%)]\tLoss: 107.308884\n",
            "Train Epoch: 10 [37120/60000 (62%)]\tLoss: 108.940178\n",
            "Train Epoch: 10 [38400/60000 (64%)]\tLoss: 106.784859\n",
            "Train Epoch: 10 [39680/60000 (66%)]\tLoss: 107.958481\n",
            "Train Epoch: 10 [40960/60000 (68%)]\tLoss: 108.736847\n",
            "Train Epoch: 10 [42240/60000 (70%)]\tLoss: 105.475624\n",
            "Train Epoch: 10 [43520/60000 (72%)]\tLoss: 108.679268\n",
            "Train Epoch: 10 [44800/60000 (75%)]\tLoss: 104.378975\n",
            "Train Epoch: 10 [46080/60000 (77%)]\tLoss: 106.664421\n",
            "Train Epoch: 10 [47360/60000 (79%)]\tLoss: 104.947014\n",
            "Train Epoch: 10 [48640/60000 (81%)]\tLoss: 102.677689\n",
            "Train Epoch: 10 [49920/60000 (83%)]\tLoss: 106.646439\n",
            "Train Epoch: 10 [51200/60000 (85%)]\tLoss: 107.880508\n",
            "Train Epoch: 10 [52480/60000 (87%)]\tLoss: 107.975571\n",
            "Train Epoch: 10 [53760/60000 (90%)]\tLoss: 107.339050\n",
            "Train Epoch: 10 [55040/60000 (92%)]\tLoss: 107.571388\n",
            "Train Epoch: 10 [56320/60000 (94%)]\tLoss: 105.882538\n",
            "Train Epoch: 10 [57600/60000 (96%)]\tLoss: 107.112976\n",
            "Train Epoch: 10 [58880/60000 (98%)]\tLoss: 97.819420\n",
            "====> Epoch: 10 Average loss: 106.2601\n",
            "====> Test set loss: 105.5114\n"
          ]
        }
      ],
      "source": [
        "import argparse\n",
        "import sys\n",
        "import torch\n",
        "import torch.utils.data\n",
        "from torch import nn, optim\n",
        "from torch.nn import functional as F\n",
        "from torchvision import datasets, transforms\n",
        "from torchvision.utils import save_image\n",
        "\n",
        "# argparse 설정\n",
        "parser = argparse.ArgumentParser(description='VAE MNIST Example (Colab-friendly)')\n",
        "parser.add_argument('--batch-size', type=int, default=128, metavar='N',\n",
        "                    help='input batch size for training (default: 128)')\n",
        "parser.add_argument('--epochs', type=int, default=10, metavar='N',\n",
        "                    help='number of epochs to train (default: 10)')\n",
        "parser.add_argument('--no-cuda', action='store_true', default=False,\n",
        "                    help='disables CUDA training')\n",
        "parser.add_argument('--no-mps', action='store_true', default=False,\n",
        "                    help='disables macOS GPU training')\n",
        "parser.add_argument('--seed', type=int, default=1, metavar='S',\n",
        "                    help='random seed (default: 1)')\n",
        "parser.add_argument('--log-interval', type=int, default=10, metavar='N',\n",
        "                    help='how many batches to wait before logging training status')\n",
        "\n",
        "# Colab/Jupyter가 자동으로 전달하는 -f 등 인자를 무시하기 위해 parse_known_args 사용\n",
        "args, _ = parser.parse_known_args()\n",
        "\n",
        "# CUDA, MPS, CPU 등 디바이스 설정\n",
        "args.cuda = not args.no_cuda and torch.cuda.is_available()\n",
        "use_mps = not args.no_mps and torch.backends.mps.is_available()\n",
        "torch.manual_seed(args.seed)\n",
        "\n",
        "if args.cuda:\n",
        "    device = torch.device(\"cuda\")\n",
        "elif use_mps:\n",
        "    device = torch.device(\"mps\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "\n",
        "kwargs = {'num_workers': 1, 'pin_memory': True} if args.cuda else {}\n",
        "\n",
        "# MNIST 데이터셋 로드\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    datasets.MNIST(\n",
        "        root='./data', train=True, download=True,\n",
        "        transform=transforms.ToTensor()\n",
        "    ),\n",
        "    batch_size=args.batch_size, shuffle=True, **kwargs\n",
        ")\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "    datasets.MNIST(\n",
        "        root='./data', train=False,\n",
        "        transform=transforms.ToTensor()\n",
        "    ),\n",
        "    batch_size=args.batch_size, shuffle=False, **kwargs\n",
        ")\n",
        "\n",
        "# VAE 모델 정의\n",
        "class VAE(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(VAE, self).__init__()\n",
        "        self.fc1 = nn.Linear(784, 400)\n",
        "        self.fc21 = nn.Linear(400, 20)  # mu\n",
        "        self.fc22 = nn.Linear(400, 20)  # logvar\n",
        "        self.fc3 = nn.Linear(20, 400)\n",
        "        self.fc4 = nn.Linear(400, 784)\n",
        "\n",
        "    def encode(self, x):\n",
        "        h1 = F.relu(self.fc1(x))\n",
        "        return self.fc21(h1), self.fc22(h1)\n",
        "\n",
        "    def reparameterize(self, mu, logvar):\n",
        "        std = torch.exp(0.5 * logvar)\n",
        "        eps = torch.randn_like(std)\n",
        "        return mu + eps * std\n",
        "\n",
        "    def decode(self, z):\n",
        "        h3 = F.relu(self.fc3(z))\n",
        "        return torch.sigmoid(self.fc4(h3))\n",
        "\n",
        "    def forward(self, x):\n",
        "        mu, logvar = self.encode(x.view(-1, 784))\n",
        "        z = self.reparameterize(mu, logvar)\n",
        "        return self.decode(z), mu, logvar\n",
        "\n",
        "model = VAE().to(device)\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
        "\n",
        "# 손실함수 (재구성오차 + KL발산)\n",
        "def loss_function(recon_x, x, mu, logvar):\n",
        "    BCE = F.binary_cross_entropy(recon_x, x.view(-1, 784), reduction='sum')\n",
        "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
        "    return BCE + KLD\n",
        "\n",
        "# 학습 함수\n",
        "def train(epoch):\n",
        "    model.train()\n",
        "    train_loss = 0\n",
        "    for batch_idx, (data, _) in enumerate(train_loader):\n",
        "        data = data.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        recon_batch, mu, logvar = model(data)\n",
        "        loss = loss_function(recon_batch, data, mu, logvar)\n",
        "        loss.backward()\n",
        "        train_loss += loss.item()\n",
        "        optimizer.step()\n",
        "        if batch_idx % args.log_interval == 0:\n",
        "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "                epoch,\n",
        "                batch_idx * len(data),\n",
        "                len(train_loader.dataset),\n",
        "                100. * batch_idx / len(train_loader),\n",
        "                loss.item() / len(data)\n",
        "            ))\n",
        "    print('====> Epoch: {} Average loss: {:.4f}'.format(\n",
        "          epoch, train_loss / len(train_loader.dataset)))\n",
        "\n",
        "# 테스트(평가) 함수\n",
        "def test(epoch):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for i, (data, _) in enumerate(test_loader):\n",
        "            data = data.to(device)\n",
        "            recon_batch, mu, logvar = model(data)\n",
        "            test_loss += loss_function(recon_batch, data, mu, logvar).item()\n",
        "            if i == 0:\n",
        "                n = min(data.size(0), 8)\n",
        "                comparison = torch.cat([\n",
        "                    data[:n],\n",
        "                    recon_batch.view(args.batch_size, 1, 28, 28)[:n]\n",
        "                ])\n",
        "                save_image(comparison.cpu(), f'results/reconstruction_{epoch}.png', nrow=n)\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "    print('====> Test set loss: {:.4f}'.format(test_loss))\n",
        "\n",
        "# 디렉토리 생성 (이미 존재하면 에러 없어도 괜찮음)\n",
        "import os\n",
        "os.makedirs('results', exist_ok=True)\n",
        "\n",
        "# 실제 학습\n",
        "for epoch in range(1, args.epochs + 1):\n",
        "    train(epoch)\n",
        "    test(epoch)\n",
        "    with torch.no_grad():\n",
        "        sample = torch.randn(64, 20).to(device)\n",
        "        sample = model.decode(sample).cpu()\n",
        "        save_image(sample.view(64, 1, 28, 28),\n",
        "                   f'results/sample_{epoch}.png')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# 모델을 평가 모드로 전환\n",
        "model.eval()\n",
        "\n",
        "# 테스트 로더에서 한 배치 가져오기\n",
        "data, _ = next(iter(test_loader))\n",
        "data = data.to(device)\n",
        "\n",
        "with torch.no_grad():\n",
        "    recon_batch, _, _ = model(data)\n",
        "\n",
        "# 시각화할 이미지 개수 (예: 8장)\n",
        "n = 8\n",
        "\n",
        "# 전체 그림(fig)과 서브플롯(axes) 생성: 2행 x n열\n",
        "fig, axes = plt.subplots(2, n, figsize=(n * 1.5, 3))\n",
        "\n",
        "for i in range(n):\n",
        "    # (1) 입력 이미지\n",
        "    axes[0, i].imshow(data[i].cpu().squeeze(), cmap='gray')\n",
        "    axes[0, i].set_title(\"Input\")\n",
        "    axes[0, i].axis('off')\n",
        "\n",
        "    # (2) 재구성된 이미지\n",
        "    recon_img = recon_batch[i].cpu().view(28, 28)\n",
        "    axes[1, i].imshow(recon_img.detach(), cmap='gray')\n",
        "    axes[1, i].set_title(\"Reconstructed\")\n",
        "    axes[1, i].axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 246
        },
        "id": "NHMg9brr_h-l",
        "outputId": "980a5fc3-c42e-49ea-cc3b-a0d936f69b8b"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1200x300 with 16 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    }
  ]
}